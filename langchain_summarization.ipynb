{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summarization with langchain\n",
    "\n",
    "Summarization of long documents is a common LLM use case. The issue that most often arises, however, is that there is a token limit for the model. (Max context window length). With langchain this can be worked around by chunking and recursive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# First lets install dependencies (make sure already installed)\n",
    "!pip3 install transformers chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the dependencies we need:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our API key, projectId and URL from .env\n",
    "load_dotenv()\n",
    "api_key = \"INSER API KEY HERE\"\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"INSERT PROJECT ID HERE\"\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can take a [stuff](https://python.langchain.com/docs/modules/chains/document/stuff) or [map reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) approach to summarizing documents. We'll start with the simpler \"stuff\". Feel free to play around with changing the document URL and inference parameters to optimize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize llm and document loader:\n",
    "print(\"Loading web document...\")\n",
    "# Try out some other documents as well\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\n",
    "doc = loader.load()\n",
    "print(\"Done.\")\n",
    "\n",
    "# You might need to tweak some of the runtime parameters to optimize the results.\n",
    "print(\"Initializing flan-t5-xxl model...\")\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205\n",
    "}\n",
    "\n",
    "flan_model = Model(\n",
    "    model_id=\"google/flan-t5-xxl\", \n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Can use 'stuff' or 'map reduce'; \n",
    "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
    "\n",
    "print(\"Running summarization task...\\n\")\n",
    "\n",
    "res = chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine several of the features we've seen previously, including prompt templates and chains. In the following block we load the document into a template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "print(\"Initializing chain...\")\n",
    "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "print(\"Stuff chain with documents...\")\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "print(\"Running summarization on stuffed document chain...\\n\")\n",
    "res = stuff_chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output above should be the same as the previous block if using the same inference parameters and document URL. Now we will use the same stuff chain method to see how it behaves with multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new article\n",
    "print(\"Loading 2nd article...\")\n",
    "loader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\n",
    "doc_2 = loader_2.load() # Returns list\n",
    "print(\"Done.\")\n",
    "\n",
    "# Combine docs\n",
    "docs = doc + doc_2\n",
    "\n",
    "print(\"Running summarization on stuffed document chain.\\n\")\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above code should result in an error to the effect of `input tokens (6953) plus prefix length (0) must be < 4096` meaning that we have exceeded the model's token input length. This brings us to the next topic, \"map reduce\" which helps us solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
    "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
